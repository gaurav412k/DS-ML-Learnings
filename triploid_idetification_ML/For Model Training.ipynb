{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a5b05d-0049-4f60-b96f-0d5dd710de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import gmean\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56851530-f970-4b23-a773-6f1b002ed219",
   "metadata": {},
   "outputs": [],
   "source": [
    "triploid_samples = [\"8A697\", \"8A698\", \"8A699\", \"8A700\", \"8A701\", \"8A702\", \"8A703\", \n",
    "                    \"8A704\", \"8A711\", \"8A731\", \"8A732\", \"8A712\"]  # \"8A712\",\n",
    "\n",
    "diploid_samples = [\n",
    "\"8A681\",\n",
    "\"8A682\",\n",
    "\"8A689\",\n",
    "\"8A690\",\n",
    "\"8A691\",\n",
    "\"8A692\",\n",
    "\"8A693\",\n",
    "\"8A694\",\n",
    "\"8A695\",\n",
    "\"8A696\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c474efa-a6a6-478e-96c4-2fefc3db13e8",
   "metadata": {},
   "source": [
    "# Training DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645d59f2-b8e4-4583-900b-4b42edff13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_samples_using_copynumber(directory):\n",
    "    \"\"\"\n",
    "    Imports data from CNV files in a specified directory, focuses on the 'copynumber' column,\n",
    "    combines them into a single DataFrame, and adds a 'sample' column to identify the source of each file.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): The path to the directory containing CNV files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the combined data focused on 'copynumber'.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "    lst = []  # To store filenames\n",
    "    empty_files = []  # To store names of empty files\n",
    "    failed_files = []  # To store names of files that failed to read\n",
    "    duplicate_samples = []  # To store names of duplicate samples\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.tsv'):  # Ensure the file is a .tsv file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            lst.append(filename)\n",
    "            \n",
    "            try:\n",
    "                # Attempt to read the TSV file\n",
    "                df = pd.read_csv(file_path, sep='\\t')\n",
    "                \n",
    "                if df.empty:\n",
    "                    empty_files.append(filename)  # Add empty file to the list\n",
    "                    continue  # Skip the empty file\n",
    "                \n",
    "                # Strip any whitespace from the column names\n",
    "                df.columns = df.columns.str.strip()\n",
    "                \n",
    "                # Add a column to identify the sample\n",
    "                sample_name = filename.replace('_l1_cnv.tsv', '')\n",
    "                df['sample'] = sample_name\n",
    "                \n",
    "                # Check if this sample is already in the combined DataFrame\n",
    "                if 'sample' in combined_df.columns and sample_name in combined_df['sample'].unique():\n",
    "                    duplicate_samples.append(sample_name)\n",
    "                \n",
    "                # Append the DataFrame to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Add failed file to list and report error\n",
    "                failed_files.append(filename)\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "        if filename.endswith('sort_read_depth.csv'):  # Ensure the file is a .tsv file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            lst.append(filename)\n",
    "            \n",
    "            try:\n",
    "                # Attempt to read the TSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if df.empty:\n",
    "                    empty_files.append(filename)  # Add empty file to the list\n",
    "                    continue  # Skip the empty file\n",
    "                \n",
    "                # Strip any whitespace from the column names\n",
    "                df.columns = df.columns.str.strip()\n",
    "                \n",
    "                # Add a column to identify the sample\n",
    "                sample_name = filename.replace('_l1.sort_read_depth.csv', '')\n",
    "                df['sample'] = sample_name\n",
    "                \n",
    "                \n",
    "                # Check if this sample is already in the combined DataFrame\n",
    "                if 'sample' in combined_df.columns and sample_name in combined_df['sample'].unique():\n",
    "                    duplicate_samples.append(sample_name)\n",
    "                \n",
    "                # Append the DataFrame to the combined DataFrame\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Add failed file to list and report error\n",
    "                failed_files.append(filename)\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    print(f\"Total files processed: {len(lst)}\")\n",
    "    print(f\"Total unique samples in DataFrame: {len(combined_df['sample'].unique()) if 'sample' in combined_df.columns else 0}\")\n",
    "    print(f\"Empty files: {empty_files}\")\n",
    "    print(f\"Failed to read files: {failed_files}\")\n",
    "    print(f\"Duplicate samples: {duplicate_samples}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Function to calculate geometric mean with handling for negative values\n",
    "def safe_gmean(x):\n",
    "    # Shift the values to make them all positive\n",
    "    min_value = x.min()\n",
    "    if min_value <= 0:\n",
    "        shift_value = abs(min_value) + 1e-10  # Adding a small constant to ensure positivity\n",
    "        x = x + shift_value  # Shift all values to positive\n",
    "    return gmean(x)\n",
    "\n",
    "def pivoting (combined_df,column_name = None): \n",
    "    # Step 1: Group by chromosome and sample, summing the 'copynumber' and other relevant columns\n",
    "    try:\n",
    "        grouped_df = combined_df.groupby([\"chromosome\", \"sample\"]).agg({\n",
    "#             column_name:  lambda x: safe_gmean(x) # Apply the safe geometric mean function\n",
    "              column_name:  \"sum\" # Apply the safe geometric mean function\n",
    "\n",
    "        }).reset_index()\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in grouping: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Pivot the DataFrame to make chromosomes as columns and samples as rows\n",
    "    try:\n",
    "        pivoted_df = grouped_df.pivot(index='sample', columns='chromosome', values=[column_name]).fillna(0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during pivoting: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Flatten the multi-level column index after pivoting\n",
    "    pivoted_df.columns = ['_'.join(col).strip() for col in pivoted_df.columns.values]\n",
    "    \n",
    "    # Step 3: Reset the index if you want 'sample' to be a column\n",
    "    pivoted_df.reset_index(inplace=True)\n",
    "    \n",
    "    return pivoted_df\n",
    "\n",
    "valid_samples = triploid_samples + diploid_samples\n",
    "\n",
    "# Add labels to each DataFrame\n",
    "def filter_and_label(df, sample_col=\"sample\"):\n",
    "    # Filter only valid samples\n",
    "    filtered_df = df[df[sample_col].isin(valid_samples)].copy()\n",
    "    \n",
    "    # Add the 'sample_type' column based on the sample name\n",
    "    filtered_df[\"sample_type\"] = filtered_df[sample_col].apply(\n",
    "        lambda x: \"triploid\" if x in triploid_samples else (\"diploid\" if x in diploid_samples\n",
    "                                                           else (\"haploid\" if x in haploid_samples else \n",
    "                                                                 \"unknown\"))\n",
    "    )\n",
    "    return filtered_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407a35f-f411-4401-bd25-1e1f1370e7ea",
   "metadata": {},
   "source": [
    "# Data to be Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79730f61-2f10-4e66-b301-57379050d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "NY_K_dir = \"/home/gaurav/Assignment/Triploidy_project/20241104_NY_K/triploidy classification/Working MOdel/copy/\"\n",
    "\n",
    "NY_K_dir_read_depth = '/home/gaurav/Assignment/Triploidy_project/20241104_NY_K/triploidy classification/Working MOdel/read/'\n",
    "\n",
    "batch_cp = import_samples_using_copynumber(NY_K_dir)\n",
    "batch_rd = import_samples_using_copynumber(NY_K_dir)\n",
    "\n",
    "cp  = pivoting(batch_cp,column_name=\"copynumber\")\n",
    "rd  = pivoting(batch_rd,column_name=\"readcount\")\n",
    "\n",
    "\n",
    "merged_df = pd.merge(cp, rd, on=[\"sample\"], suffixes=('_copynumber', 'readcount'))\n",
    "\n",
    "merged_df[\"sample_type\"] = merged_df['sample'].apply(\n",
    "    lambda x: \"triploid\" if x in triploid_samples else (\"diploid\" if x in diploid_samples\n",
    "                                                       else (\"haploid\" if x in haploid_samples else \n",
    "                                                             \"unknown\"))\n",
    ")\n",
    "\n",
    "merged_df = merged_df[merged_df[\"sample_type\"]!=\"unknown\"].reset_index(drop = True)\n",
    "\n",
    "merged_filterd_df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c2dd1-5fd4-43c6-af81-c27afa68cfbd",
   "metadata": {},
   "source": [
    "#  Performing the Training model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991714b7-c76b-4a0a-a00d-ee8b0e584825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "# Retain the 'sample' column for evaluation\n",
    "samples = merged_filterd_df[\"sample\"]\n",
    "\n",
    "# Encode sample_type (target)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(merged_filterd_df[\"sample_type\"])  # Encode target (diploid)\n",
    "\n",
    "# Features (X)\n",
    "chromosome_columns = [col for col in merged_filterd_df.columns if col.startswith((\"read_depth\",\"copynumber\"))]\n",
    "X = merged_filterd_df[chromosome_columns]  # Fixed order of chromosome features\n",
    "\n",
    "# Step 2: Standardize the data (important for models like Logistic Regression, Gradient Boosting)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale the entire dataset\n",
    "\n",
    "\n",
    "# Step 3: Train the model (use the entire dataset for training)\n",
    "# No need for train-test split, we'll use the entire dataset\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Step 4: Evaluate the model on the same data (since you're training with the full dataset)\n",
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "# Step 5: Performance metrics\n",
    "print(f\"Model Classification Report:\\n\")\n",
    "print(classification_report(y, y_pred, target_names=label_encoder.classes_))\n",
    "print(f\"Accuracy: {accuracy_score(y, y_pred):.2f}\")\n",
    "\n",
    "# Step 6: Attach predictions back to the dataset for sample-wise evaluation\n",
    "predicted_sample_types = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Combine results for sample-wise evaluation\n",
    "results_df = X.copy()\n",
    "results_df[\"sample\"] = samples.reset_index(drop=True)\n",
    "results_df[\"actual_sample_type\"] = label_encoder.inverse_transform(y)\n",
    "results_df[\"predicted_sample_type\"] = predicted_sample_types\n",
    "\n",
    "# Print the first few results\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02a8c8-c51b-492e-a21e-e53049c6e670",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed03c1a6-3214-4004-84c8-6a5f07c46f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Step 7: Save the model, scaler, and label encoder\n",
    "model_dict = {\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': label_encoder\n",
    "}\n",
    "\n",
    "# Save the model and scaler as a dictionary\n",
    "joblib.dump(model_dict, 'model_and_scaler_labelencoder_1.pkl')\n",
    "\n",
    "print(\"Model, scaler, and label encoder saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
